import os
import pandas as pd
from evaluation import *
import random
import copy
from keras.preprocessing import text, sequence
import torch
from torch import nn
from torch.utils import data
from torch.nn import functional as F
import numpy as np
import time
import math
import gc
import re


class SpatialDropout(nn.Dropout2d):
    def forward(self, x):
        x = x.unsqueeze(2)  # (N, T, 1, K)
        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)
        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked
        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)
        x = x.squeeze(2)  # (N, T, K)
        return x


class NeuralNet(nn.Module):
    def __init__(self, embedding_matrix):
        super(NeuralNet, self).__init__()
        unique_word_num = embedding_matrix.shape[0]
        embed_size = embedding_matrix.shape[1]
        lstm_size = 128
        dense_size = 512
        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(unique_word_num, embed_size)
        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False
        self.embedding_dropout = SpatialDropout(0.3)
        # LSTM
        self.lstm1 = nn.LSTM(embed_size, lstm_size, bidirectional=True, batch_first=True)
        self.lstm2 = nn.LSTM(lstm_size * 2, lstm_size, bidirectional=True, batch_first=True)
        # å…¨è¿æ¥å±‚
        self.linear1 = nn.Linear(dense_size, dense_size)
        self.linear2 = nn.Linear(dense_size, dense_size)
        self.linear3 = nn.Linear(dense_size * 2, dense_size)
        # è¾“å‡ºå±‚
        self.linear_out = nn.Linear(dense_size, 1)
        self.linear_aux_out = nn.Linear(dense_size, 5)
        self.linear_identity_out = nn.Linear(dense_size, 9)
        self.linear_identity_out2 = nn.Linear(dense_size, dense_size)
        self.bn1 = nn.BatchNorm1d(dense_size)
        self.bn2 = nn.BatchNorm1d(dense_size)

    def forward(self, x):
        # åµŒå…¥å±‚
        h_embedding = self.embedding(x)
        h_embedding = self.embedding_dropout(h_embedding)
        # LSTM
        h_lstm1, _ = self.lstm1(h_embedding)
        h_lstm2, _ = self.lstm2(h_lstm1)
        # pooling
        avg_pool = torch.mean(h_lstm2, 1)
        max_pool, _ = torch.max(h_lstm2, 1)
        # å…¨è¿æ¥å±‚
        h_conc = torch.cat((max_pool, avg_pool), 1)

        identity_hidden = self.linear_identity_out2(h_conc)
        identity_hidden = F.relu(identity_hidden)
        #identity_hidden = self.bn1(identity_hidden)
        identity_hidden = F.dropout(identity_hidden, p=0.3)
        identity_result = self.linear_identity_out(identity_hidden)
        h_conc2 = torch.cat((h_conc, identity_hidden), 1)
        gate_hidden = self.linear3(h_conc2)
        #gate_hidden = self.bn2(gate_hidden)
        gate = torch.sigmoid(gate_hidden)
        #gate = F.dropout(gate, p=0.3)
        h_conc = h_conc * gate

        h_conc_linear1 = F.relu(self.linear1(h_conc))
        h_conc_linear2 = F.relu(self.linear2(h_conc))
        # æ‹¼æ¥
        hidden = h_conc + h_conc_linear1 + h_conc_linear2
        # è¾“å‡ºå±‚ï¼Œç”¨ sigmoid å°±ç”¨ BCELossï¼Œä¸ç”¨ sigmoid å°±ç”¨ BCEWithLogitsLoss
        result = self.linear_out(hidden)
        aux_result = self.linear_aux_out(hidden)
        out = torch.cat([result, aux_result, identity_result], 1)
        return out


class Trainer:
    def __init__(self, data_dir, model_name, epochs=5, batch_size=512, part=1., debug_mode=False):
        self.data_dir = data_dir
        self.debug_mode = debug_mode
        self.model_name = model_name
        self.identity_list = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']
        self.toxicity_type_list = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']
        self.weight_dict = {"severe_toxicity": 1000, "obscene": 195, "identity_attack": 277, "insult": 21,
                            "threat": 608, "male": 44, "female": 32, "homosexual_gay_or_lesbian": 197, "christian": 47,
                            "jewish": 242, "muslim": 132, "black": 130, "white": 89, "psychiatric_or_mental_illness": 368,
                            "np": 12, "pn": 15}
        self.stopwords = '!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\nâ€œâ€â€™\'âˆÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\Ã—â„¢âˆšÂ²â€”'
        self.seed_everything()
        self.seed = 5
        self.max_len = 220
        self.epochs = epochs
        self.batch_size = batch_size
        self.split_ratio = 0.95
        self.sample_num = 1804874
        if not self.debug_mode:
            self.train_df = pd.read_csv(os.path.join(self.data_dir, "train.csv")).head(int(self.sample_num * part)).fillna(0.)
            self.test_df = pd.read_csv(os.path.join(self.data_dir, "test.csv"))
        else:
            self.train_df = pd.read_csv(os.path.join(self.data_dir, "train.csv")).head(1000).fillna(0.)
            self.test_df = pd.read_csv(os.path.join(self.data_dir, "test.csv")).head(1000)
        self.train_len = int(len(self.train_df) * self.split_ratio)
        self.evaluator = self.init_evaluator()
        self.mapping_dict = None
        self.contraction_mapping = None
        self.contraction_re = None
        self.init_text_cleaner()

    def seed_everything(self, seed=1234):
        random.seed(seed)
        os.environ['PYTHONHASHSEED'] = str(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True

    def init_evaluator(self):
        # åˆå§‹åŒ–è¯„åˆ†å‡½æ•°ç±»
        y_true = self.train_df['target'].values
        y_identity = self.train_df[self.identity_list].values
        valid_y_true = y_true[self.train_len:]
        valid_y_identity = y_identity[self.train_len:]
        evaluator = JigsawEvaluator(valid_y_true, valid_y_identity) # y_true å¿…é¡»æ˜¯0æˆ–1ï¼Œä¸èƒ½æ˜¯ç¦»æ•£å€¼
        return evaluator

    def init_text_cleaner(self):
        # æœ‰è¯å‘é‡çš„ç‰¹æ®Šå­—ç¬¦è¿›è¡Œéš”ç¦»ï¼Œæ²¡æœ‰çš„è¿›è¡Œåˆ é™¤
        symbols_to_isolate = '.,?!-;*"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\x96\x92â—Â£â™¥â¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûâ€ Î¼âœ’â¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼â¬…â„…Â»Ğ’Ğ°Ğ²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—â–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹â¡Â«Ï†â…“â€âœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑĞ¸Ê¿âœ¨ã€‚É‘\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜âœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼Ê•ÉÌ£Î”â‚€âœâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜ï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'
        symbols_to_delete = '\nğŸ•\rğŸµğŸ˜‘\xa0\ue014\t\uf818\uf04a\xadğŸ˜¢ğŸ¶ï¸\uf0e0ğŸ˜œğŸ˜ğŸ‘Š\u200b\u200eğŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ğŸ˜ğŸ’–ğŸ’µĞ•ğŸ‘ğŸ˜€ğŸ˜‚\u202a\u202cğŸ”¥ğŸ˜„ğŸ»ğŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ğŸ˜‹ğŸ‘×©×œ×•××‘×™ğŸ˜±â€¼\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\u2009ğŸšŒá´µÍğŸŒŸğŸ˜ŠğŸ˜³ğŸ˜§ğŸ™€ğŸ˜ğŸ˜•\u200fğŸ‘ğŸ˜®ğŸ˜ƒğŸ˜˜××¢×›×—ğŸ’©ğŸ’¯â›½ğŸš„ğŸ¼à®œğŸ˜–á´ ğŸš²â€ğŸ˜ŸğŸ˜ˆğŸ’ªğŸ™ğŸ¯ğŸŒ¹ğŸ˜‡ğŸ’”ğŸ˜¡\x7fğŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ğŸ™„ï¼¨ğŸ˜ \ufeff\u2028ğŸ˜‰ğŸ˜¤â›ºğŸ™‚\u3000ØªØ­ÙƒØ³Ø©ğŸ‘®ğŸ’™ÙØ²Ø·ğŸ˜ğŸ¾ğŸ‰ğŸ˜\u2008ğŸ¾ğŸ˜…ğŸ˜­ğŸ‘»ğŸ˜¥ğŸ˜”ğŸ˜“ğŸ½ğŸ†ğŸ»ğŸ½ğŸ¶ğŸŒºğŸ¤”ğŸ˜ª\x08â€‘ğŸ°ğŸ‡ğŸ±ğŸ™†ğŸ˜¨ğŸ™ƒğŸ’•ğ˜Šğ˜¦ğ˜³ğ˜¢ğ˜µğ˜°ğ˜¤ğ˜ºğ˜´ğ˜ªğ˜§ğ˜®ğ˜£ğŸ’—ğŸ’šåœ°ç„è°·ÑƒĞ»ĞºĞ½ĞŸĞ¾ĞĞğŸ¾ğŸ•ğŸ˜†×”ğŸ”—ğŸš½æ­Œèˆä¼ğŸ™ˆğŸ˜´ğŸ¿ğŸ¤—ğŸ‡ºğŸ‡¸Ğ¼Ï…Ñ‚Ñ•â¤µğŸ†ğŸƒğŸ˜©\u200ağŸŒ ğŸŸğŸ’«ğŸ’°ğŸ’ÑĞ¿Ñ€Ğ´\x95ğŸ–ğŸ™…â›²ğŸ°ğŸ¤ğŸ‘†ğŸ™Œ\u2002ğŸ’›ğŸ™ğŸ‘€ğŸ™ŠğŸ™‰\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\x13ğŸš¬ğŸ¤“\ue602ğŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª××“×£× ×¨×š×¦×˜ğŸ˜’ÍğŸ†•ğŸ‘…ğŸ‘¥ğŸ‘„ğŸ”„ğŸ”¤ğŸ‘‰ğŸ‘¤ğŸ‘¶ğŸ‘²ğŸ”›ğŸ“\uf0b7\uf04c\x9f\x10æˆéƒ½ğŸ˜£âºğŸ˜ŒğŸ¤‘ğŸŒğŸ˜¯ĞµÑ…ğŸ˜²á¼¸á¾¶á½ğŸ’ğŸš“ğŸ””ğŸ“šğŸ€ğŸ‘\u202dğŸ’¤ğŸ‡\ue613å°åœŸè±†ğŸ¡â”â‰\u202fğŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ğŸ‡¹ğŸ‡¼ğŸŒ¸è”¡è‹±æ–‡ğŸŒğŸ²ãƒ¬ã‚¯ã‚µã‚¹ğŸ˜›å¤–å›½äººå…³ç³»Ğ¡Ğ±ğŸ’‹ğŸ’€ğŸ„ğŸ’œğŸ¤¢ÙÙÑŒÑ‹Ğ³Ñä¸æ˜¯\x9c\x9dğŸ—‘\u2005ğŸ’ƒğŸ“£ğŸ‘¿à¼¼ã¤à¼½ğŸ˜°á¸·Ğ—Ğ·â–±Ñ†ï¿¼ğŸ¤£å–æ¸©å“¥åè®®ä¼šä¸‹é™ä½ å¤±å»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨éª—å­ğŸãƒ„ğŸ…\x85ğŸºØ¢Ø¥Ø´Ø¡ğŸµğŸŒÍŸá¼”æ²¹åˆ«å…‹ğŸ¤¡ğŸ¤¥ğŸ˜¬ğŸ¤§Ğ¹\u2003ğŸš€ğŸ¤´Ê²ÑˆÑ‡Ğ˜ĞĞ Ğ¤Ğ”Ğ¯ĞœÑĞ¶ğŸ˜ğŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ğŸ’¨åœ†æ˜å›­×§â„ğŸˆğŸ˜ºğŸŒâá»‡ğŸ”ğŸ®ğŸğŸ†ğŸ‘ğŸŒ®ğŸŒ¯ğŸ¤¦\u200dğ“’ğ“²ğ“¿ğ“µì•ˆì˜í•˜ì„¸ìš”Ğ–Ñ™ĞšÑ›ğŸ€ğŸ˜«ğŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æğŸ¼ğŸ•ºğŸ¸ğŸ¥‚ğŸ—½ğŸ‡ğŸŠğŸ†˜ğŸ¤ ğŸ‘©ğŸ–’ğŸšªå¤©ä¸€å®¶âš²\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒğŸ‡«ğŸ‡·ğŸ‡©ğŸ‡ªğŸ‡®ğŸ‡¬ğŸ‡§ğŸ˜·ğŸ‡¨ğŸ‡¦Ğ¥Ğ¨ğŸŒ\x1fæ€é¸¡ç»™çŒ´çœ‹Êğ—ªğ—µğ—²ğ—»ğ˜†ğ—¼ğ˜‚ğ—¿ğ—®ğ—¹ğ—¶ğ˜‡ğ—¯ğ˜ğ—°ğ˜€ğ˜…ğ—½ğ˜„ğ—±ğŸ“ºÏ–\u2000Ò¯Õ½á´¦á¥Ò»Íº\u2007Õ°\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆğ“ğ¡ğğ«ğ®ğğšğƒğœğ©ğ­ğ¢ğ¨ğ§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ğ†á´‘Üğ¬ğ°ğ²ğ›ğ¦ğ¯ğ‘ğ™ğ£ğ‡ğ‚ğ˜ğŸÔœĞ¢á—à±¦ã€”á«ğ³ğ”ğ±ğŸ”ğŸ“ğ…ğŸ‹ï¬ƒğŸ’˜ğŸ’“Ñ‘ğ˜¥ğ˜¯ğ˜¶ğŸ’ğŸŒ‹ğŸŒ„ğŸŒ…ğ™¬ğ™–ğ™¨ğ™¤ğ™£ğ™¡ğ™®ğ™˜ğ™ ğ™šğ™™ğ™œğ™§ğ™¥ğ™©ğ™ªğ™—ğ™ğ™ğ™›ğŸ‘ºğŸ·â„‹ğ€ğ¥ğªğŸš¶ğ™¢á¼¹ğŸ¤˜Í¦ğŸ’¸Ø¬íŒ¨í‹°ï¼·ğ™‡áµ»ğŸ‘‚ğŸ‘ƒÉœğŸ«\uf0a7Ğ‘Ğ£Ñ–ğŸš¢ğŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ğŸƒğ“¬ğ“»ğ“´ğ“®ğ“½ğ“¼â˜˜ï´¾Ì¯ï´¿â‚½\ue807ğ‘»ğ’†ğ’ğ’•ğ’‰ğ’“ğ’–ğ’‚ğ’ğ’…ğ’”ğ’ğ’—ğ’ŠğŸ‘½ğŸ˜™\u200cĞ›â€’ğŸ¾ğŸ‘¹âŒğŸ’â›¸å…¬å¯“å…»å® ç‰©å—ğŸ„ğŸ€ğŸš‘ğŸ¤·æ“ç¾ğ’‘ğ’šğ’ğ‘´ğŸ¤™ğŸ’æ¬¢è¿æ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ğ™«ğŸˆğ’Œğ™Šğ™­ğ™†ğ™‹ğ™ğ˜¼ğ™…ï·»ğŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ğŸš—ğŸ³ğŸğŸğŸ–ğŸ‘ğŸ•ğ’„ğŸ—ğ ğ™„ğ™ƒğŸ‘‡é”Ÿæ–¤æ‹·ğ—¢ğŸ³ğŸ±ğŸ¬â¦ãƒãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ğ˜¿ğ™”â‚µğ’©â„¯ğ’¾ğ“ğ’¶ğ“‰ğ“‡ğ“Šğ“ƒğ“ˆğ“…â„´ğ’»ğ’½ğ“€ğ“Œğ’¸ğ“ğ™Î¶ğ™Ÿğ˜ƒğ—ºğŸ®ğŸ­ğŸ¯ğŸ²ğŸ‘‹ğŸ¦Šå¤šä¼¦ğŸ½ğŸ»ğŸ¹â›“ğŸ¹ğŸ·ğŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸å…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ğŸ¸ğŸ¤•ğŸ¤’â›‘ğŸæ‰¹åˆ¤æ£€è®¨ğŸğŸ¦ğŸ™‹ğŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ì˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ğŸ”«ğŸ‘å‡¸á½°ğŸ’²ğŸ—¯ğ™ˆá¼Œğ’‡ğ’ˆğ’˜ğ’ƒğ‘¬ğ‘¶ğ•¾ğ–™ğ–—ğ–†ğ–ğ–Œğ–ğ–•ğ–Šğ–”ğ–‘ğ–‰ğ–“ğ–ğ–œğ–ğ–šğ–‡ğ•¿ğ–˜ğ–„ğ–›ğ–’ğ–‹ğ–‚ğ•´ğ–Ÿğ–ˆğ•¸ğŸ‘‘ğŸš¿ğŸ’¡çŸ¥å½¼ç™¾\uf005ğ™€ğ’›ğ‘²ğ‘³ğ‘¾ğ’‹ğŸ’ğŸ˜¦ğ™’ğ˜¾ğ˜½ğŸğ˜©ğ˜¨á½¼á¹‘ğ‘±ğ‘¹ğ‘«ğ‘µğ‘ªğŸ‡°ğŸ‡µğŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘á“€á£ğŸ„ğŸˆğŸ”¨ğŸğŸ¤ğŸ¸ğŸ’ŸğŸ°ğŸŒğŸ›³ç‚¹å‡»æŸ¥ç‰ˆğŸ­ğ‘¥ğ‘¦ğ‘§ï¼®ï¼§ğŸ‘£\uf020ã£ğŸ‰Ñ„ğŸ’­ğŸ¥ÎğŸ´ğŸ‘¨ğŸ¤³ğŸ¦\x0bğŸ©ğ‘¯ğ’’ğŸ˜—ğŸğŸ‚ğŸ‘³ğŸ—ğŸ•‰ğŸ²Ú†ÛŒğ‘®ğ—•ğ—´ğŸ’êœ¥â²£â²ğŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ğŸ’Šã€Œã€\uf203\uf09a\uf222\ue608\uf202\uf099\uf469\ue607\uf410\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆĞ“ğ‘©ğ‘°ğ’€ğ‘ºğŸŒ¤ğ—³ğ—œğ—™ğ—¦ğ—§ğŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ğŸ‡³ğ’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ğ’ğŸ”¹ğŸ¤šğŸğ‘·ğŸ‚ğŸ’…ğ˜¬ğ˜±ğ˜¸ğ˜·ğ˜ğ˜­ğ˜“ğ˜–ğ˜¹ğ˜²ğ˜«Ú©Î’ÏğŸ’¢ÎœÎŸÎÎ‘Î•ğŸ‡±â™²ğˆâ†´ğŸ’’âŠ˜È»ğŸš´ğŸ–•ğŸ–¤ğŸ¥˜ğŸ“ğŸ‘ˆâ•ğŸš«ğŸ¨ğŸŒ‘ğŸ»ğğğŠğ‘­ğŸ¤–ğŸğŸ˜¼ğŸ•·ï½‡ï½’ï½ï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ğŸ°ğŸ‡´ğŸ‡­ğŸ‡»ğŸ‡²ğ—ğ—­ğ—˜ğ—¤ğŸ‘¼ğŸ“‰ğŸŸğŸ¦ğŸŒˆğŸ”­ã€ŠğŸŠğŸ\uf10aáƒšÚ¡ğŸ¦\U0001f92f\U0001f92ağŸ¡ğŸ’³á¼±ğŸ™‡ğ—¸ğ—Ÿğ— ğ—·ğŸ¥œã•ã‚ˆã†ãªã‚‰ğŸ”¼'
        small_caps_mapping = {"á´€": "a", "Ê™": "b", "á´„": "c", "á´…": "d", "á´‡": "e", "Ò“": "f", "É¢": "g", "Êœ": "h", "Éª": "i", "á´Š": "j", "á´‹": "k", "ÊŸ": "l", "á´": "m", "É´": "n", "á´": "o", "á´˜": "p", "Ç«": "q", "Ê€": "r", "s": "s", "á´›": "t", "á´œ": "u", "á´ ": "v", "á´¡": "w", "x": "x", "Ê": "y", "á´¢": "z"}
        contraction_mapping = {
            "ain't": "is not", "aren't": "are not", "can't": "cannot", "'cause": "because", "could've": "could have",
            "couldn't": "could not",
            "didn't": "did not", "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not",
            "haven't": "have not",
            "he'd": "he would", "he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you",
            "how'll": "how will", "how's": "how is",
            "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have", "I'm": "I am",
            "I've": "I have", "i'd": "i would", "i'd've":
                "i would have", "i'll": "i will", "i'll've": "i will have", "i'm": "i am", "i've": "i have",
            "isn't": "is not", "it'd": "it would",
            "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have", "it's": "it is",
            "let's": "let us", "ma'am": "madam",
            "mayn't": "may not", "might've": "might have", "mightn't": "might not", "mightn't've": "might not have",
            "must've": "must have",
            "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have",
            "o'clock": "of the clock", "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not",
            "sha'n't": "shall not", "shan't've": "shall not have", "she'd": "she would", "she'd've": "she would have",
            "she'll": "she will", "she'll've": "she will have", "she's": "she is", "should've": "should have",
            "shouldn't": "should not",
            "shouldn't've": "should not have", "so've": "so have", "so's": "so as", "this's": "this is",
            "that'd": "that would",
            "that'd've": "that would have", "that's": "that is", "there'd": "there would",
            "there'd've": "there would have", "there's": "there is",
            "here's": "here is", "they'd": "they would", "they'd've": "they would have", "they'll": "they will",
            "they'll've": "they will have",
            "they're": "they are", "they've": "they have", "to've": "to have", "wasn't": "was not", "we'd": "we would",
            "we'd've": "we would have",
            "we'll": "we will", "we'll've": "we will have", "we're": "we are", "we've": "we have",
            "weren't": "were not", "what'll": "what will",
            "what'll've": "what will have", "what're": "what are", "what's": "what is", "what've": "what have",
            "when's": "when is",
            "when've": "when have", "where'd": "where did", "where's": "where is", "where've": "where have",
            "who'll": "who will", "who'll've": "who will have",
            "who's": "who is", "who've": "who have", "why's": "why is", "why've": "why have", "will've": "will have",
            "won't": "will not",
            "won't've": "will not have", "would've": "would have", "wouldn't": "would not",
            "wouldn't've": "would not have",
            "y'all": "you all", "y'all'd": "you all would", "y'all'd've": "you all would have",
            "y'all're": "you all are", "y'all've": "you all have",
            "you'd": "you would", "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",
            "you're": "you are", "you've": "you have",
            "trump's": "trump is", "obama's": "obama is", "canada's": "canada is", "today's": "today is"}
        # å­—ç¬¦çš„æ›¿æ¢å­—å…¸çš„ key è¦è½¬æˆ ascii ç æ‰èƒ½ç”¨ translate
        isolate_dict = {ord(c): f' {c} ' for c in symbols_to_isolate}
        remove_dict = {ord(c): f'' for c in symbols_to_delete}
        small_caps_mapping_dict = {ord(k): v for k, v in small_caps_mapping.items()}
        mapping_dict = {}
        mapping_dict.update(isolate_dict)
        mapping_dict.update(remove_dict)
        mapping_dict.update(small_caps_mapping_dict)
        contraction_re = re.compile('(%s)' % '|'.join(contraction_mapping.keys()))
        self.mapping_dict = mapping_dict
        self.contraction_mapping = contraction_mapping
        self.contraction_re = contraction_re

    def clean_text(self, x):
        # å°†ç®€å†™å±•å¼€
        x = self.contraction_re.sub(lambda match: self.contraction_mapping[match.group(0)], x)
        # éš”ç¦»å’Œåˆ é™¤ç‰¹æ®Šå­—ç¬¦
        x = x.translate(self.mapping_dict)
        # åˆ é™¤è¯è¯­å‰çš„å¼•å·
        x = x.split(" ")
        x = [x_[1:] if x_.startswith("'") else x_ for x_ in x]
        x = ' '.join(x)
        return x

    def create_dataloader(self):
        # è¯»å–è¾“å…¥è¾“å‡º
        #train_comments = self.train_df["comment_text"].astype(str)
        train_comments = self.train_df["comment_text"].astype(str).apply(lambda x: self.clean_text(x))
        train_label = self.train_df["target"].values
        train_type_labels = self.train_df[self.toxicity_type_list].values

        # èº«ä»½åŸå§‹å€¼
        train_identity_values = self.train_df[self.identity_list].fillna(0.).values
        # æ‰€æœ‰èº«ä»½åŸå§‹å€¼ä¹‹å’Œ
        train_identity_sum = train_identity_values.sum(axis=1)
        # å°†èº«ä»½ä¹‹å’Œé™åˆ¶åœ¨1ä»¥ä¸‹ï¼ˆsigmoidï¼‰
        train_identity_sum_label = np.where(train_identity_sum > 1, 1, train_identity_sum)
        # èº«ä»½01å€¼
        train_identity_binary = copy.deepcopy(self.train_df[self.identity_list])
        for column in self.identity_list:
            train_identity_binary[column] = np.where(train_identity_binary[column] > 0.5, 1, 0)
        # èº«ä»½01å€¼æœ‰ä¸€ä¸ªå°±ç®—1
        train_identity_binary_sum = train_identity_binary.sum(axis=1)
        train_identity_or_binary = np.where(train_identity_binary_sum >= 1, 1, 0)
        # æ‰€æœ‰èº«ä»½æ ‡ç­¾
        train_identity_type_labels = train_identity_values
        train_identity_type_binary_lables = train_identity_binary
        train_identity_sum_label = train_identity_sum_label
        train_identity_binary_label = train_identity_or_binary

        # tokenizer è®­ç»ƒ
        # æ¸…ç†æ•°æ®
        #test_comments = self.test_df["comment_text"].astype(str)
        test_comments = self.test_df["comment_text"].astype(str).apply(lambda x: self.clean_text(x))
        #tokenizer = text.Tokenizer(filters=self.stopwords)
        # å¦‚æœ filter ä¸ç½®ä¸ºç©ºï¼Œä¼šè‡ªåŠ¨åˆ æ‰å¾ˆå¤šå­—ç¬¦ï¼Œlower ä¸ç½®ä¸º False ä¼šè‡ªåŠ¨å…¨éƒ¨å°å†™
        tokenizer = text.Tokenizer(filters='', lower=False)
        tokenizer.fit_on_texts(list(train_comments) + list(test_comments))    # train_comments æ˜¯ dataframe çš„ä¸€åˆ—ï¼Œæ˜¯ Series ç±»ï¼Œ list(train_comments) ç›´æ¥å˜æˆ list
        # tokenization
        train_tokens = tokenizer.texts_to_sequences(train_comments)     # å¯ä»¥ç»™ Series ä¹Ÿå¯ä»¥ç»™ listï¼Ÿ
        test_tokens = tokenizer.texts_to_sequences(test_comments)
        # ç”¨ sequence ç±»è¡¥åˆ°å®šé•¿
        train_tokens = sequence.pad_sequences(train_tokens, maxlen=self.max_len)
        test_tokens = sequence.pad_sequences(test_tokens, maxlen=self.max_len)
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        valid_tokens = train_tokens[self.train_len:]
        valid_label = train_label[self.train_len:]
        valid_type_labels = train_type_labels[self.train_len:]
        train_tokens = train_tokens[:self.train_len]
        train_label = train_label[:self.train_len]
        train_type_labels = train_type_labels[:self.train_len]
        valid_identity_type_labels = train_identity_type_labels[self.train_len:]
        train_identity_type_labels = train_identity_type_labels[:self.train_len]
        valid_identity_type_binary_lables = train_identity_type_binary_lables[self.train_len:]
        train_identity_type_binary_lables = train_identity_type_binary_lables[:self.train_len]
        valid_identity_sum_label = train_identity_sum_label[self.train_len:]
        train_identity_sum_label = train_identity_sum_label[:self.train_len]
        valid_identity_binary_label = train_identity_binary_label[self.train_len:]
        train_identity_binary_label = train_identity_binary_label[:self.train_len]

        # è®¡ç®—æ ·æœ¬æƒé‡
        target_weight, aux_weight, identity_weight = self.cal_sample_weights()

        # å°†ç¬¦å·åŒ–æ•°æ®è½¬æˆ tensor
        train_x_tensor = torch.tensor(train_tokens, dtype=torch.long)
        valid_x_tensor = torch.tensor(valid_tokens, dtype=torch.long)
        train_y_tensor = torch.tensor(np.hstack([train_label[:, np.newaxis], train_type_labels, train_identity_type_labels]), dtype=torch.float32)
        valid_y_tensor = torch.tensor(np.hstack([valid_label[:, np.newaxis], valid_type_labels, valid_identity_type_labels]), dtype=torch.float32)
        target_weight_tensor = torch.tensor(target_weight, dtype=torch.float32)
        aux_weight_tensor = torch.tensor(aux_weight, dtype=torch.float32)
        identity_weight_tensor = torch.tensor(identity_weight, dtype=torch.float32)
        if torch.cuda.is_available():
            train_x_tensor = train_x_tensor.cuda()
            valid_x_tensor = valid_x_tensor.cuda()
            train_y_tensor = train_y_tensor.cuda()
            valid_y_tensor = valid_y_tensor.cuda()
            target_weight_tensor = target_weight_tensor.cuda()
            aux_weight_tensor = aux_weight_tensor.cuda()
            identity_weight_tensor = identity_weight_tensor.cuda()
        # å°† tensor è½¬æˆ datasetï¼Œè®­ç»ƒæ•°æ®å’Œæ ‡ç­¾ä¸€ä¸€å¯¹åº”ï¼Œç”¨ dataloader åŠ è½½çš„æ—¶å€™ dataset[:-1] æ˜¯ xï¼Œdataset[-1] æ˜¯ y
        train_dataset = data.TensorDataset(train_x_tensor, train_y_tensor, target_weight_tensor, aux_weight_tensor, identity_weight_tensor)
        valid_dataset = data.TensorDataset(valid_x_tensor, valid_y_tensor)
        # å°† dataset è½¬æˆ dataloader
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False)
        # è¿”å›è®­ç»ƒæ•°æ®
        return train_loader, valid_loader, tokenizer

    def cal_sample_weights(self):
        # aux weight
        aux_weight = np.zeros((len(self.train_df), len(self.toxicity_type_list)))
        for i, column in enumerate(self.toxicity_type_list):
            weight = math.pow(self.weight_dict[column], 0.5)
            aux_weight[:, i] = np.where(self.train_df[column] > 0.5, weight, 1)
        # identity weight
        identity_weight = np.zeros((len(self.train_df), len(self.identity_list)))
        for i, column in enumerate(self.identity_list):
            weight = math.pow(self.weight_dict[column], 0.5)
            identity_weight[:, i] = np.where(self.train_df[column] > 0.5, weight, 1)
        # target weight
        for column in self.identity_list + ["target"]:
            self.train_df[column] = np.where(self.train_df[column] > 0.5, True, False)
        target_weight = np.ones(len(self.train_df))
        target_weight += self.train_df["target"]
        if False:
            target_weight += (~self.train_df["target"]) * self.train_df[self.identity_list].sum(axis=1)
            target_weight += self.train_df["target"] * (~self.train_df[self.identity_list]).sum(axis=1) * 5
        else:
            target_weight += (~self.train_df["target"]) * np.where(self.train_df[self.identity_list].sum(axis=1) > 0, 1, 0) * 3
            target_weight += self.train_df["target"] * np.where((~self.train_df[self.identity_list]).sum(axis=1) > 0, 1, 0) * 3
        target_weight /= target_weight.mean()
        # åªç•™è®­ç»ƒé›†
        target_weight = target_weight[:self.train_len]
        aux_weight = aux_weight[:self.train_len, :]
        identity_weight = identity_weight[:self.train_len, :]
        return target_weight, aux_weight, identity_weight

    def create_emb_weights(self, word_index):
        # æ„å»ºè¯å‘é‡å­—å…¸
        with open(os.path.join(self.data_dir, "crawl-300d-2M.vec"), "r") as f:
            fasttext_emb_dict = {}
            for i, line in enumerate(f):
                if i == 1000 and self.debug_mode: break
                split = line.strip().split(" ")
                word = split[0]
                if word not in word_index: continue
                emb = np.array([float(num) for num in split[1:]])
                fasttext_emb_dict[word] = emb
        with open(os.path.join(self.data_dir, "glove.840B.300d.txt"), "r") as f:
            glove_emb_dict = {}
            for i, line in enumerate(f):
                if i == 1000 and self.debug_mode: break
                split = line.strip().split(" ")
                word = split[0]
                if word not in word_index: continue
                emb = np.array([float(num) for num in split[1:]])
                glove_emb_dict[word] = emb
        # ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†å‡ºç°è¿‡çš„è¯æ„å»ºè¯å‘é‡çŸ©é˜µ
        word_embedding = np.zeros((len(word_index) + 1, 600))     # tokenizer è‡ªåŠ¨ç•™å‡º0ç”¨æ¥ padding
        np.random.seed(1234)
        fasttext_random_emb = np.random.uniform(-0.25, 0.25, 300)   # ç”¨äº fasttext æ‰¾ä¸åˆ°è¯è¯­æ—¶
        np.random.seed(1235)
        glove_random_emb = np.random.uniform(-0.25, 0.25, 300)  # ç”¨äº glove æ‰¾ä¸åˆ°è¯è¯­æ—¶
        for word, index in word_index.items():
            # å¦‚æœæ‰¾ä¸åˆ° embï¼Œå°è¯•å°å†™æˆ–é¦–å­—æ¯å¤§å†™
            if word not in fasttext_emb_dict and word not in glove_emb_dict:
                word = word.lower()
                if word not in fasttext_emb_dict and word not in glove_emb_dict:
                    word = word.title()
                    if word not in fasttext_emb_dict and word not in glove_emb_dict:
                        word = word.upper()
            fasttext_emb = fasttext_emb_dict[word] if word in fasttext_emb_dict else fasttext_random_emb
            glove_emb = glove_emb_dict[word] if word in glove_emb_dict else glove_random_emb
            word_embedding[index] = np.concatenate((fasttext_emb, glove_emb), axis=-1)
        return np.array(word_embedding)

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def custom_loss(self, y_pred, y_batch, target_weight=1., aux_weight=1., identity_weight=1.):
        target_pred = y_pred[:, 0]
        target_true = y_batch[:, 0]
        aux_pred = y_pred[:, 1: 6]
        aux_true = y_batch[:, 1: 6]
        identity_pred = y_pred[:, 6:]
        identity_true = y_batch[:, 6:]
        target_loss = nn.BCEWithLogitsLoss(reduction="none")(target_pred, target_true)
        target_loss = torch.mean(target_loss * target_weight)
        aux_loss = nn.BCEWithLogitsLoss(reduction="none")(aux_pred, aux_true)
        aux_loss = torch.mean(aux_loss * aux_weight)
        identity_loss = nn.BCEWithLogitsLoss(reduction="none")(identity_pred, identity_true)
        identity_loss = torch.mean(identity_loss * identity_weight)
        return target_loss, aux_loss, identity_loss

    def train(self):
        if self.debug_mode: self.epochs = 1
        # åŠ è½½ dataloader
        train_loader, valid_loader, tokenizer = self.create_dataloader()
        # ç”Ÿæˆ embedding
        word_embedding = self.create_emb_weights(tokenizer.word_index)
        # è®­ç»ƒ
        self.seed_everything(1234)
        model = NeuralNet(word_embedding)
        if torch.cuda.is_available():
            model.cuda()
        lr = 1e-3
        # param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()] # å¯ä»¥ä¸ºä¸åŒå±‚è®¾ç½®ä¸åŒçš„å­¦ä¹ é€Ÿç‡
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        # æ¸å˜å­¦ä¹ é€Ÿç‡
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)
        # æŸå¤±å‡½æ•°
        loss_fn = nn.BCEWithLogitsLoss(reduction='mean')
        # è®­ç»ƒ
        previous_auc_score = 0
        stop_flag = 0
        for epoch in range(self.epochs):
            start_time = time.time()
            # è°ƒæ•´ä¸€æ¬¡å­¦ä¹ é€Ÿç‡
            scheduler.step()
            # åˆ‡æ¢ä¸ºè®­ç»ƒæ¨¡å¼
            model.train()
            # åˆå§‹åŒ–å½“å‰ epoch çš„ loss
            avg_loss = 0.
            # åŠ è½½æ¯ä¸ª batch å¹¶è®­ç»ƒ
            for batch_data in train_loader:
                x_batch = batch_data[0]
                y_batch = batch_data[1]
                target_weight_batch = batch_data[2]
                aux_weight_batch = batch_data[3]
                identity_weight_batch = batch_data[4]
                #y_pred = model(*x_batch)
                y_pred = model(x_batch)
                target_loss, aux_loss, identity_loss = self.custom_loss(y_pred, y_batch, target_weight_batch, aux_weight_batch, identity_weight_batch)
                loss = target_loss + aux_loss + identity_loss
                #loss = loss_fn(y_pred, y_batch)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                avg_loss += loss.item() / len(train_loader)
            # è®¡ç®—éªŒè¯é›†
            model.eval()
            y_pred = np.zeros((len(self.train_df) - self.train_len))
            for i, batch_data in enumerate(valid_loader):
                x_batch = batch_data[:-1]
                y_batch = batch_data[-1]
                batch_y_pred = self.sigmoid(model(*x_batch).detach().cpu().numpy())[:, 0]
                y_pred[i * self.batch_size: (i + 1) * self.batch_size] = batch_y_pred
            # è®¡ç®—å¾—åˆ†
            auc_score = self.evaluator.get_final_metric(y_pred)
            print("epoch: %d duration: %d min auc_score: %.4f" % (epoch, int((time.time() - start_time) / 60), auc_score))
            if not self.debug_mode and epoch > 0:
                temp_dict = model.state_dict()
                del temp_dict['embedding.weight']
                torch.save(temp_dict, os.path.join(self.data_dir, "model/model[pytorch][%s]_%d_%.5f" % (self.model_name, epoch, auc_score)))
        # del è®­ç»ƒç›¸å…³è¾“å…¥å’Œæ¨¡å‹
        training_history = [train_loader, valid_loader, tokenizer, word_embedding, model, optimizer, scheduler]
        for variable in training_history:
            del variable
        gc.collect()


if __name__ == "__main__":
    data_dir = "/Users/hedongfeng/PycharmProjects/unintended_bias/data/"
    trainer = Trainer(data_dir, "model_name", debug_mode=True)
    trainer.train()
